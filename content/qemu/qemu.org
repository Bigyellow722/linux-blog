#+title: how to use qemu
#+author: lhkwqy
#+date:


* qemu monitor

** basic operations

- enter monitor mode

  #+begin_src comment
    Ctrl+a c
  #+end_src


* Device Emulation

** Device Front End

** Device Back End

** Device Pass Through


* Qtest




* recommendation for KVM CPU model configuration on x86 host

** Two way to configure CPU models with QEMU/KVM

1. Host passthrough

   #+begin_src shell
     # enabling host passthrough need -enable_kvm option.
     qemu-system-x86_64 -cpu host

     # host passthrough with feature customization:
     qemu-system-x86_64 -cpu host, vmx=off,...
   #+end_src

2. Named model

   #+begin_src shell
     qemu-system-x86_64 -cpu Westmere

     # Named model with feature customization
     qemu-system-x86_64 -cpu Westmere,pcid=on,...
   #+end_src

** Preferred CPU models for Intel x86 hosts

|---+---+---+---+---+---+---+---+---+---+---+---|
|  |  |  |  |  |  |  |  |  |  |  |
|---+---+---+---+---+---+---+---+---+---+---+---|
|option | Cascadelake-Server, Cascadelake-Server-noTSX  | Skylake-Server, Skylake-Server-IBRS, Skylake-Server-IBRS-noTSX | Skylake-Client, Skylake-Client-IBRS, Skylake-Client-noTSX-IBRS  | Broadwell, Broadwell-IBRS, Broadwell-noTSX, Broadwell-noTSX-IBRS  | Haswell, Haswell-IBRS, Haswell-noTSX, Haswell-noTSX-IBRS  | IvyBridge, IvyBridge-IBR | SandyBridge, SandyBridge-IBRS | estmere, Westmere-IBRS  | Nehalem, Nehalem-IBRS  | Penryn  | Conroe  |
|---+---+---+---+---+---+---+---+---+---+---+---|
|model | Intel Xeon Processor (Cascade Lake, 2019) | Intel Xeon Processor (Skylake, 2016)  | Intel Core Processor (Skylake, 2015)  | Intel Core Processor (Broadwell, 2014)  | Intel Core Processor (Haswell, 2013)  | Intel Xeon E3-12xx v2 (Ivy Bridge, 2012)  | Intel Xeon E312xx (Sandy Bridge, 2011)  | Westmere E56xx/L56xx/X56xx (Nehalem-C, 2010)  | Intel Core i7 9xx (Nehalem Class Core i7, 2008)  | Intel Core 2 Duo P9xxx (Penryn Class Core 2, 2007)  | Intel Celeron_4x0 (Conroe/Merom Class Core 2, 2006)  |
|---+---+---+---+---+---+---+---+---+---+---+---|

** add numa node

#+begin_src shell
  -numa node[,mem=size][,cpus=firstcpu[-lastcpu]][,nodeid=node][,initiator=initiator]

  -numa node[,memdev=id][,cpus=firstcpu[-lastcpu]][,nodeid=node][,initiator=initiator]

  -numa dist,src=source,dst=destination,val=distance

  -numa cpu,node-id=node[,socket-id=x][,core-id=y][,thread-id=z]

  -numa hmat-lb,initiator=node,target=node,hierarchy=hierarchy,data-type=type[,latency=lat][,bandwidth=bw]

  -numa hmat-cache,node-id=node,size=size,level=level[,associativity=str][,policy=str][,line=size]
#+end_src

examples:

the following option assigns VCPUs 0, 1, 2 and 5 to a NUMA node:

#+begin_src shell
  -numa node,cpus=0-2,cpus=5
#+end_src

Following example creates a machine with 2 NUMA nodes, node 0 has CPU. node 1 has only memory, and its initiator is node 0. Note that because node 0 has CPU, by default the initiator of node 0 is itself and must be itself.

#+begin_src shell
  -machine hmat=on \
  -m 2G,slots=2,maxmem=4G \
  -object memory-backend-ram,size=1G,id=m0 \
  -object memory-backend-ram,size=1G,id=m1 \
  -numa node,nodeid=0,memdev=m0 \
  -numa node,nodeid=1,memdev=m1,initiator=0 \
  -smp 2,sockets=2,maxcpus=2  \
  -numa cpu,node-id=0,socket-id=0 \
  -numa cpu,node-id=0,socket-id=1
#+end_src

For example, the following options describe 2 NUMA nodes. Node 0 has 2 cpus and a ram, node 1 has only a ram. The processors in node 0 access memory in node 0 with access-latency 5 nanoseconds, access-bandwidth is 200 MB/s; The processors in NUMA node 0 access memory in NUMA node 1 with access-latency 10 nanoseconds, access-bandwidth is 100 MB/s. And for memory side cache information, NUMA node 0 and 1 both have 1 level memory cache, size is 10KB, policy is write-back, the cache Line size is 8 bytes

#+begin_src shell
-machine hmat=on \
-m 2G \
-object memory-backend-ram,size=1G,id=m0 \
-object memory-backend-ram,size=1G,id=m1 \
-smp 2,sockets=2,maxcpus=2 \
-numa node,nodeid=0,memdev=m0 \
-numa node,nodeid=1,memdev=m1,initiator=0 \
-numa cpu,node-id=0,socket-id=0 \
-numa cpu,node-id=0,socket-id=1 \
-numa hmat-lb,initiator=0,target=0,hierarchy=memory,data-type=access-latency,latency=5 \
-numa hmat-lb,initiator=0,target=0,hierarchy=memory,data-type=access-bandwidth,bandwidth=200M \
-numa hmat-lb,initiator=0,target=1,hierarchy=memory,data-type=access-latency,latency=10 \
-numa hmat-lb,initiator=0,target=1,hierarchy=memory,data-type=access-bandwidth,bandwidth=100M \
-numa hmat-cache,node-id=0,size=10K,level=1,associativity=direct,policy=write-back,line=8 \
-numa hmat-cache,node-id=1,size=10K,level=1,associativity=direct,policy=write-back,line=8
#+end_src
